<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Artificial Neural Network (ANN) From Scratch | Machine Learning and Artificial Intelligence Journey</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Artificial Neural Network (ANN) From Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog post is a way to record my ML and AI journey. It includes longer technical writeups of the projects, as well as shorter but more theoretical posts outlining my understnading or intuition of some concepts. I hope it can be helpful to people who are also on a similar journey!" />
<meta property="og:description" content="This blog post is a way to record my ML and AI journey. It includes longer technical writeups of the projects, as well as shorter but more theoretical posts outlining my understnading or intuition of some concepts. I hope it can be helpful to people who are also on a similar journey!" />
<link rel="canonical" href="https://nikita-yatchenko.github.io/blog/2021/01/31/BYO-Artificial-Neural-Network.html" />
<meta property="og:url" content="https://nikita-yatchenko.github.io/blog/2021/01/31/BYO-Artificial-Neural-Network.html" />
<meta property="og:site_name" content="Machine Learning and Artificial Intelligence Journey" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-31T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://nikita-yatchenko.github.io/blog/2021/01/31/BYO-Artificial-Neural-Network.html","@type":"BlogPosting","headline":"Artificial Neural Network (ANN) From Scratch","dateModified":"2021-01-31T00:00:00-06:00","datePublished":"2021-01-31T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nikita-yatchenko.github.io/blog/2021/01/31/BYO-Artificial-Neural-Network.html"},"description":"This blog post is a way to record my ML and AI journey. It includes longer technical writeups of the projects, as well as shorter but more theoretical posts outlining my understnading or intuition of some concepts. I hope it can be helpful to people who are also on a similar journey!","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nikita-yatchenko.github.io/blog/feed.xml" title="Machine Learning and Artificial Intelligence Journey" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Machine Learning and Artificial Intelligence Journey</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Artificial Neural Network (ANN) From Scratch</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-31T00:00:00-06:00" itemprop="datePublished">
        Jan 31, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      21 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/nikita-yatchenko/blog/tree/master/_notebooks/2021-01-31-BYO-Artificial-Neural-Network.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/nikita-yatchenko/blog/master?filepath=_notebooks%2F2021-01-31-BYO-Artificial-Neural-Network.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/nikita-yatchenko/blog/blob/master/_notebooks/2021-01-31-BYO-Artificial-Neural-Network.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-31-BYO-Artificial-Neural-Network.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Defining-a-Structure-of-our-Neural-Network">1. Defining a Structure of our Neural Network<a class="anchor-link" href="#1.-Defining-a-Structure-of-our-Neural-Network"> </a></h2><h3 id="1.1-Simplified-Intuition">1.1 Simplified Intuition<a class="anchor-link" href="#1.1-Simplified-Intuition"> </a></h3><p>What is a neural network? A neural network is a collection of connected nodes, which loosely remind a structure of a human brain. When receiving an input (a signal) the network passes it to a node, which processes it and passes processed signal to a next neuron. Each time we pass processed input to a new neuron, we learn something new about about our input. This information can then be used to better understand a relationship between an input and an output. Let us assume that we have some observed input and for each input we have some output. Neural Network, can desribe the nature of the relationship between the input and an ouput, learning both linear and non-linear components of that relationship. <strong>Neural Networks help us find linear and non-linear relationship between an input, $X$, and an output, $Y$. $$Y = f(X)$$</strong></p>
<p>To make things very simple, imagine your input as a single number $x$. When we use this input in our network, we pass $x$ to a neuron, call it $a$. It will $process$ the input, $x$, to, hopefully, produce an insight from that input, which will allow us to learn something of value about our output $y$. Let us call a processed input from a neuron (aka the insight) $a_l$, where $l$ is a layer of our NN, and define it as follows: $a_l = f_l(w_{l} * x + b_{l})$. Let us call the linear transformation step $z_l = w_{l} * x + b_{l}$ - simply doing that will be equivalent to performing a very large OLS (if there are multiple neurons). The advantage of using a neural network is that it allows us to capture not only a linear relatinoship (OLS) between an input and an output, but also a non-linear one. That is why we have $f_l$ function applied to calculating $a_l$. This allows us to approximate a behavior of infinitely complex functions, given, enough of neurons and layers of neurons are used.</p>
<h3 id="1.2-General-Case">1.2 General Case<a class="anchor-link" href="#1.2-General-Case"> </a></h3><p>To translate an above simplified explanation to a matrix format, let us first define what an input is, $X$. This matrix is comprised of rows and columns. Where each row is an observation, and each column is a particular feature. Columns can be words and rows can be sentences, columns can be house features (like, distance from center, number of bedrooms, etc.) and rows can be individual houses. 
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/X_mat.jpg" alt="" />
    
    
</figure>
</p>
<p><caption><center> **Figure 1**</center></caption><br />
What a Neural Netork does is it takes features ($x_1$, $x_2$,..$x_n$ - columns) and those vectors become the input. At each layer of the network, the network 'learns' new patterns from the input, by means of combining various features together - $z_l = w_{l} * x_i + b_{l}$ and applying a non-linear function to detect non-linear patterns: $a_l = f_l(x_i * w_{l} + b_{l})$, where an example of $f_l()$ could be $ReLU$ function. For the general case there can be multiple layers, $l$. Each layer consists of multiple neurons $a_l$'s that are stacked in that layer. To get from layer $l$ to layer $l+1$ in the network, we treat $a_l$'s as inputs and $a_{l+1}$ as our outputs - and repeat the process. THe  intuition behind doing this is as we move up the layers of our Network we learn more complex patterns - details of that relationship.
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/NN picture.jpg" alt="" />
    
    
</figure>
</p>
<p><caption><center> **Figure 2**</center></caption><br />
Note that the dimensions of each layer are not necessarily the same: $n ?= h ?= k ?= l$. That's because it is up to us, the architects of the Neural Network to determine the best structure, depending on the problem.</p>
<p>Now, we have input observations, $X$, and output observations, $Y$. The training of the Neural Network is figuring out the most optimal weights, $w_a$'s above. Those weights are the weights for the various features, that will become new features in the next layer.</p>
<p>How do we figure out weights? Well, just like a child learning how to walk, our Neural Network will attempt to predict out output with random guess, and will most likely fail. BUT we will gather important information - a measure of difference between our output $Y$, and what our network predicted $\hat{Y}$ - called <strong>the loss</strong>. We can then figure out how to vary the weights, $W_l$'s, such that we minimize the loss. In other words, at each iteration of training we find the derivative of the loss function with respect to each $W_l$ and $b_l$, those derivatives show the direction of greatest increase of the loss function. Therefore, for the next iteration, we are going to take <strong>negative</strong> of that derivative - to make it a direction of steepest descent - and make <strong>some</strong> step (<strong>learning rate</strong>) in that direction.</p>
<p>We figure out those weights by employing <strong>Stochastic Gradient Descent</strong> (SGD) algorithm. SGD is an involved topic, and we can spend a whole new notebook on it. THe main idea of the SGD is that it.</p>
<p>This was a very simple and intuitive overview of Neural Networks. The above is a very crude way to sum up WHAT Neural Networks do and HOW they accomplish it. For each sentence there are blogs, books and tutorials.</p>
<h2 id="1.3-Outline-of-This-Project.-Implement-L-layer-NN">1.3 Outline of This Project. Implement L-layer NN<a class="anchor-link" href="#1.3-Outline-of-This-Project.-Implement-L-layer-NN"> </a></h2><p>Again the goal is to implement our own Artificial Neural Network in Python. Here is the basic outline:</p>
<ol>
<li>Initialize the parameters</li>
<li>Propagation<ol>
<li>Compute Forward Propagation</li>
<li>Copmute the Loss Function</li>
<li>Compute Backward Propagation</li>
</ol>
</li>
<li>Update parameters</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Coding">2. Coding<a class="anchor-link" href="#2.-Coding"> </a></h2><h3 id="2.1-Initialize-the-parameters">2.1 Initialize the parameters<a class="anchor-link" href="#2.1-Initialize-the-parameters"> </a></h3><p>To begin with, we are going to initialize our parameters. There is a lot of theory behind how to properly initialize parameters, but the main conclusion is that $b$ can be initialized to a vector of all 0's, but $W$ cannot, otherwise, the NN will not be able to break symmetry and no learning will be done.</p>
<p>Let us next deal with the dimensions of our $W$ and $b$ parameters. For some layer $l$, we have $W_l$, which takes observations in the space of $R_{l-1}$ and transforms them to $R_{l}$. Consider the first step of neural network, input to the first layer:
$$ A1_{m, h}  = X_{m, n}  \times W1_{n, h}  + b_{1, h} $$, where $m$ is the number of observations, $n$ is the number of features in the input layer, h is the number of features in the first layer, $W1$ has dimension $(n, h)$, aka <strong>old dimensions</strong> to <strong>new dimensions</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">from</span> <span class="nn">testCases_v4a</span> <span class="kn">import</span> <span class="o">*</span> <span class="c1"># testing taken from Andrew Ng&#39;s course</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As input we are given a list , containins the depth of each layer. Thus the length of the list is the total number of layers, and the value of each element is the depth (number of new features, aka h).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">layers_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">    input:</span>
<span class="sd">        - layer_list - list containing the depth of each layer</span>
<span class="sd">    notes:</span>
<span class="sd">        - W_l - has dimensions (layers_list[l-1], layers_list[l])</span>
<span class="sd">        - b_l - has dimensions (1, layers_list[l])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_list</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_list</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">layers_list</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span> <span class="c1"># need to multiply by a small number to avoid exploding gradients</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers_list</span><span class="p">[</span><span class="n">l</span><span class="p">]])</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">initialize_parameters</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;W_1&#39;: array([[ 0.01788628,  0.0043651 ,  0.00096497, -0.01863493],
        [-0.00277388, -0.00354759, -0.00082741, -0.00627001],
        [-0.00043818, -0.00477218, -0.01313865,  0.00884622],
        [ 0.00881318,  0.01709573,  0.00050034, -0.00404677],
        [-0.0054536 , -0.01546477,  0.00982367, -0.01101068]]),
 &#39;b_1&#39;: array([[0., 0., 0., 0.]]),
 &#39;W_2&#39;: array([[-0.01185047, -0.0020565 ,  0.01486148],
        [ 0.00236716, -0.01023785, -0.00712993],
        [ 0.00625245, -0.00160513, -0.00768836],
        [-0.00230031,  0.00745056,  0.01976111]]),
 &#39;b_2&#39;: array([[0., 0., 0.]])}</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.2-Propagation">2.2 Propagation<a class="anchor-link" href="#2.2-Propagation"> </a></h3><p>Now, after we have initialized our parameters we need to propagate forward to calculate our error (loss) and then we need to propagate backward to calculate gradients at each step. We will use those gradients to modify our parameters.</p>
<p><figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/propagation.jpg" alt="" />
    
    
</figure>
</p>
<p><caption><center> **Figure 3**</center></caption><br /></p>
<h4 id="2.2.A-Forward-Propagation">2.2.A Forward Propagation<a class="anchor-link" href="#2.2.A-Forward-Propagation"> </a></h4><p>First, we implement forward propagation</p>
<p>Forward Propagation:</p>
<ol>
<li>$Z_l = A_{l-1} \times W_l + b_l$ - this is the linear part of the neuron calculation</li>
<li>$A_l = f_l(Z_l)$ - this is the non-linear part of the neuron calculation. $f_l$ most often are ReLU or sigmoid</li>
</ol>
<p>To correctly implement backward propagation later on, we need to cache some intemediate values for later use. Namely, we need to cache: $Z_l$, $A_{l-1}$, $W_l$, and $b_l$.</p>
<p>Before we jump into forward propagation, let's quickly define ReLU (Rectified Linear Unit) and sigmoid functions. ReLU takes only the positive part of the array, aka $max(0,Z)$. While sigmoid is this: $\frac{1}{1+e^{-Z}}$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">relu_forward</span><span class="p">(</span><span class="n">Z_l</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a forward activate function ReLU </span>
<span class="sd">    input:</span>
<span class="sd">        - Z_l - a linear part of the neuron calculation</span>
<span class="sd">    output:</span>
<span class="sd">        - A_l - a nonlinear part of the neuron calculation, after applying relu activation function to Z_curr</span>
<span class="sd">        - cache - cached value of Z_curr to be used later in the backward propagation step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">A_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Z_l</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">A_l</span>
<span class="c1"># print(relu_forward(np.array([ -1,   4,   5, -40])))</span>

<span class="k">def</span> <span class="nf">sigmoid_forward</span><span class="p">(</span><span class="n">Z_l</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a forward activate function sigmoid </span>
<span class="sd">    input:</span>
<span class="sd">        - Z_l - a linear part of the neuron calculation</span>
<span class="sd">    output:</span>
<span class="sd">        - A_l - a nonlinear part of the neuron calculation, after applying sigmoid activation function to Z_l</span>
<span class="sd">        - cache - cached value of Z_l to be used later in the backward propagation step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">A_l</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Z_l</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">A_l</span>

<span class="c1"># print(sigmoid_forward(np.array([ -1,   4,   5, -40])))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As was outline in Forward Propagation section above, we just need to implement that in code and cache important intermidiate values:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W_l</span><span class="p">,</span> <span class="n">b_l</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a single step of forward propagation, taking A_{l-1} and producing A_l</span>
<span class="sd">    </span>
<span class="sd">    input:</span>
<span class="sd">        - A_prev - previous layer </span>
<span class="sd">        - W_l, b_l - weights for the current to next layer</span>
<span class="sd">        - activation - activation function for the layer</span>
<span class="sd">    output:</span>
<span class="sd">        - A_curr - current layer</span>
<span class="sd">        - cache - storing A_prev, W_l, b_l, Z_l for later use in back prop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">Z_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W_l</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_l</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
        <span class="n">A_curr</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">Z_l</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">:</span>
        <span class="n">A_curr</span> <span class="o">=</span> <span class="n">sigmoid_forward</span><span class="p">(</span><span class="n">Z_l</span><span class="p">)</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">[</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W_l</span><span class="p">,</span> <span class="n">b_l</span><span class="p">,</span> <span class="n">Z_l</span><span class="p">,</span> <span class="n">activation</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">A_curr</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">A_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.41675785</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1361961</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.79343559</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.05626683</span><span class="p">,</span>  <span class="mf">1.64027081</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.84174737</span><span class="p">]])</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.50288142</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.24528809</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.05795222</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.90900761</span><span class="p">]])</span>
<span class="n">forward_propagation</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(array([[0.96890023],
        [0.11013289]]),
 [array([[-0.41675785, -2.1361961 , -1.79343559],
         [-0.05626683,  1.64027081, -0.84174737]]),
  array([[ 0.50288142],
         [-1.24528809],
         [-1.05795222]]),
  array([[-0.90900761]]),
  array([[ 3.43896134],
         [-2.08938436]]),
  &#39;sigmoid&#39;])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that this is a single forward propagation step, for the full pass through the model we need to loop through the entire structure. This is exactly what we will implement now:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">L_layer_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies forward propagation for the entire network</span>
<span class="sd">    input:</span>
<span class="sd">        - X - training feature data</span>
<span class="sd">        - parameters - all initialized parameters</span>
<span class="sd">    output:</span>
<span class="sd">        - A_last - last layer activated neurons</span>
<span class="sd">        - caches - a list of caches for each layer of the network</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">caches</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">X</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span>
        <span class="n">A_prev</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">A_prev</span>
    
    <span class="n">W</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span>
    <span class="n">A_L</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>

    <span class="k">assert</span><span class="p">(</span><span class="n">A_L</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">A_L</span><span class="p">,</span> <span class="n">caches</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.31178367</span><span class="p">,</span>  <span class="mf">0.72900392</span><span class="p">,</span>  <span class="mf">0.21782079</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8990918</span> <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.48678065</span><span class="p">,</span>  <span class="mf">0.91325152</span><span class="p">,</span>  <span class="mf">1.12706373</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.51409323</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.63929108</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4298936</span> <span class="p">,</span>  <span class="mf">2.63128056</span><span class="p">,</span>  <span class="mf">0.60182225</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.33588161</span><span class="p">,</span>  <span class="mf">1.23773784</span><span class="p">,</span>  <span class="mf">0.11112817</span><span class="p">,</span>  <span class="mf">0.12915125</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.07612761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15512816</span><span class="p">,</span>  <span class="mf">0.63422534</span><span class="p">,</span>  <span class="mf">0.810655</span>  <span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L_layer_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[0.49999998],
       [0.4999998 ],
       [0.50000061],
       [0.5       ],
       [0.50000011]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">L_model_forward_test_case_2hidden</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">new_parameters</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">new_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">name</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">AL</span><span class="p">,</span> <span class="n">caches</span> <span class="o">=</span> <span class="n">L_layer_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">new_parameters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AL = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">AL</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Length of caches list = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>AL = [[0.03921668]
 [0.70498921]
 [0.19734387]
 [0.04728177]]
Length of caches list = 3
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table style="width:50%">
  <tr>
    <td> **AL** </td>
    <td> [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> 
  </tr>
  <tr>
    <td> **Length of caches list ** </td>
    <td> 3 </td> 
  </tr>
</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.2.B-Cost-Function">2.2.B Cost Function<a class="anchor-link" href="#2.2.B-Cost-Function"> </a></h4><p>As we mentioned earlier, the goal is to update parameters, based on how the cost function is reacting to changes in parameters. Therefore, defining a cost function is one of thekeys to having a successful Neural Network. There are many choices of such functions, depending on the problem's specifications:</p>
<ol>
<li>If we have a categorization problem, then a good choice can be cross entropy loss, defined as $J = -1/m\sum_{i = 0}^{m}(\sum_{k = 1}^{K}t_i[k]*log(f(s)_i[k]))$, where $t$ is truth and $s$ is the output of the NN, and m is the total number of observations.</li>
<li>If we have a regression problem ($Y_{true}$ is continuous), then a good choice can be MSE loss, defined as $J = 1/m*\sum_{i = 1}^{m}(y_i - \hat{y}_i)^2$, where m is the number of observations.</li>
</ol>
<p>backward propagation's purpose is to obtain parameters' derivatives.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;categorical&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes average cost for all training examples using either cross entropy or MSE loss, depending on the specialization</span>
<span class="sd">    input:</span>
<span class="sd">        - A_L - final layer of the network</span>
<span class="sd">        - Y - true y values</span>
<span class="sd">        - loss - categorical or continuous</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;categorical&#39;</span><span class="p">:</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A_L</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A_L</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;continuous&#39;</span><span class="p">:</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">A_L</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
    
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>      <span class="c1"># To make sure your cost&#39;s shape is what we expect (e.g. this turns [[17]] into 17)</span>
    
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span><span class="p">,</span> <span class="n">AL</span> <span class="o">=</span> <span class="n">compute_cost_test_case</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cost = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>cost = 0.2797765635793422
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.2.C-Backward-Propagation">2.2.C Backward Propagation<a class="anchor-link" href="#2.2.C-Backward-Propagation"> </a></h4><p>Now for the last part of this exercise - to build a backward propagation to calculate our derivatives. Remember our visual representation of propagation:
<figure>
  
    <img class="docimage" src="/blog/images/copied_from_nb/images/propagation.jpg" alt="" />
    
    
</figure>
</p>
<p><caption><center> **Figure 3**</center></caption><br /></p>
<p>Let's approach this first theoretically. Remember that we have the following from our forward propagation step:</p>
<ol>
<li>$Z_l = A_{l-1} \times W_l + b_l$ (Equation 1)</li>
<li>$A_l = f_l(Z_l)$, where $f_l$ is some activation function (in our case ReLU or sigmoid)</li>
<li>After going through the entire network we also have $J$, a cost function, which is a measure of difference between $A_L$ and $Y$, thus $J \approx A_L$.</li>
</ol>
<p>We want to find the following derivatives by applying the chain rule:</p>
<ol>
<li>$\frac{\delta J}{\delta W_l} = \frac{\delta J}{\delta A_L} \times \frac{\delta A_L}{\delta A_{L-1}} \times \frac{\delta A_{L-1}}{\delta A_{L-2}} ... \times \frac{\delta A_{l+1}}{\delta A_{l}} \times \frac{\delta A_{l}}{\delta Z_{l}} \times \frac{\delta Z_{l}}{\delta W_{l}} = \frac{\delta J}{\delta Z_l} \times \frac{\delta Z_l}{\delta W_l} = dZ_l \times \frac{\delta Z_l}{\delta W_l} = dZ_l \times A_{l-1}$</li>
</ol>
<p>What is $\frac{\delta Z_l}{\delta W_l}$ equal to? From Equation 1 we can find the derivative of $Z_l$ with respect to $W_l$:
$\frac{\delta Z_l}{\delta W_l} = \frac{\delta (A_{l-1} \times W_l + b_l)}{\delta W_l} = A_{l-1} $</p>
<p>So we need to understand what $dZ_l$ equal to</p>
<ol>
<li>Now, let's derive $dZ_l = \frac{\delta J}{\delta Z_l} = \frac{\delta J}{\delta A_L} \times \frac{\delta A_L}{\delta A_{L-1}} \times \frac{\delta A_{L-1}}{\delta A_{L-2}} ... \times \frac{\delta A_{l+1}}{\delta A_{l}} \times \frac{\delta A_{l}}{\delta Z_{l}} = \frac{\delta J}{\delta A_l} \times \frac{\delta A_{l}}{\delta Z_{l}} = dA_l * f'_l(Z_l)$ (Equation 2)</li>
</ol>
<p>Note that $dA_l = \frac{\delta J}{\delta A_l}$, $f'_l(Z_l)$ is a derivative of the activation function in layer $l$ evaluated for $Z_l$, and $*$ is a term-by-term multiplication of matricies.</p>
<ol>
<li>The complicated part from above is $dA_l$. Let's derive it using chain rule: </li>
</ol>
<p>$dA_l = \frac{\delta J}{\delta A_l} = \frac{\delta J}{\delta A_L} \times \frac{\delta A_L}{\delta A_{L-1}} \times \frac{\delta A_{L-1}}{\delta A_{L-2}} ... \times \frac{\delta A_{l+1}}{\delta Z_{l+1}} \times \frac{\delta Z_{l+1}}{\delta A_{l}} =  dZ_{l+1} \times \frac{\delta Z_{l+1}}{\delta A_{l}} = dZ_{l+1} \times W_{l+1}$</p>
<p>Note that $\frac{\delta Z_{l}}{\delta A_{l-1}} = W_l$, since $Z_l = A_{l-1} \times W_l + b_l$</p>
<p>Using our newly derived $dA_l$ we substitute it back to Equation 2 to obtain: $dZ_l =  dZ_{l+1} \times W_{l+1} * f'_l(Z_l)$</p>
<ol>
<li>$\frac{\delta J}{\delta b_l} = \frac{\delta J}{\delta A_L} \times \frac{\delta A_L}{\delta A_{L-1}} \times \frac{\delta A_{L-1}}{\delta A_{L-2}} ... \times \frac{\delta A_{l+1}}{\delta A_{l}} \times \frac{\delta A_{l}}{\delta Z_{l}} \times \frac{\delta Z_{l}}{\delta b_{l}} = \frac{\delta J}{\delta Z_l} \times \frac{\delta Z_l}{\delta b_l} = dZ_l \times \frac{\delta Z_l}{\delta b_l} = dZ_l \times 1$, </li>
</ol>
<p>where $1$ is the identity matrix since $Z_l = A_{l-1} \times W_l + b_l$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.2.5.1-Calculation-Order">2.2.5.1 Calculation Order<a class="anchor-link" href="#2.2.5.1-Calculation-Order"> </a></h4><p>Let's break this up into several steps. We are half way through backward propagation - at layer $l+1$. We have calculated $dA_{l+1}, dW_{l+1}, db_{l+1}, dZ_{l+1}$. We want to get $dA_{l}, dW_{l}, db_{l}, dZ_{l}$:</p>
<p>Function single<em>backward - propagates through one layer in the network using $dZ</em>{l+1}$, $W_{l+1}$ to get $dZ_l$ and $dA_l$, utilizing $f'_l(Z_l)$</p>
<p>When we have classification with 2 classes, our final activation function is 'sigmoid' function, which squeeshes activated neurons to be between values of 0 and 1. But for the rest of the layer we can choose either ReLU or 'sigmoid'.</p>
<p>Below is the implementation of the function single_backward. The calculation logic is the same, however, the calculations depend on what activation function was used for that layer. Let's consider two activation functions:</p>
<ol>
<li>ReLU - it is an identity function, when the value is greater or equal to $0$, but when the input is less than $0$, the output is $0$. Thus the derivative is $1$ for values greater than $0$, and $0$ for values that are less than $0$.</li>
<li>Sigmoid is usually used as the last activation function for the last layer. If our cost function is cross-entropy and we have a binary classification ($K = 2$), then $J = -1/m\sum_{i = 0}^{m}(y_i*log(AL_i) + (1- y_i)*log(1-AL_i)$. Let us first compute derivative calculation on a single observation and then generalize for all by summing over $m$ and taking the average $\frac{1}{m}$. So consider one example: $J_i = -((y_i*log(AL_i) + (1 - y_i)*log(1-AL_i))$, then $\frac{\delta J_i}{\delta AL_i} = \frac{AL_i - y_i}{AL_i(1-AL_i)}$</li>
</ol>
<p>So on the case of the binary classification $f_L(Z_L) = A_L = \frac{1}{1+e^{-Z_L}}$, then $f'_L(Z_L) = (\frac{1}{1+e^{-Z_L}})' = ((1+e^{-Z_L})^{-1})' = -1\times-1\times e^{-Z_L}\times \frac{1}{(1+e^{-Z_L})^{-2}} = \frac{e^{-Z_L}}{1+e^{-Z_L}} \times \frac{1}{1+e^{-Z_L}} = (1- \frac{1}{1+e^{-Z_L}})\times \frac{1}{1+e^{-Z_L}} = (1-f_L(Z_L)) \times f_L(Z_L) = (1-A_L) \times A_L$</p>
<p>Thus $\frac{\delta J_i}{\delta ZL_i} = \frac{\delta J_i}{\delta AL_i} \times \frac{\delta AL_i}{\delta ZL_i} = \frac{AL_i - y_i}{AL_i(1-AL_i)}  * f'_L(Z_L) = \frac{AL_i - y_i}{AL_i(1-AL_i)} \times (1-AL_i) \times AL_i = AL_i - y_i$.</p>
<p>Conclusion is that $dZ_L = \frac{1}{m} \times (A_L - Y)$ for the last layer.</p>
<p>If it is not the last layer, then $dZ_l = dA_l \times (1-A_l) \times A_l$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">single_backward</span><span class="p">(</span><span class="n">dA_l</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given dA_l, cache - finds the value of dA_prev based on what kind of activation was used.</span>
<span class="sd">    input:</span>
<span class="sd">        - dA_l is the derivative of J with respect to current hidden layer A_curr</span>
<span class="sd">        - cache is a list of A_prev, W_l, b_l, Z_l, activation</span>
<span class="sd">    output:</span>
<span class="sd">        - dA_prev</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">A_prev</span><span class="p">,</span> <span class="n">W_l</span><span class="p">,</span> <span class="n">b_l</span><span class="p">,</span> <span class="n">Z_l</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
        <span class="n">relu_deriv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">Z_l</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fill_value</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">relu_deriv</span><span class="p">[</span><span class="n">Z_l</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">dZ_l</span> <span class="o">=</span> <span class="n">dA_l</span> <span class="o">*</span> <span class="n">relu_deriv</span>
        <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ_l</span><span class="p">,</span> <span class="n">W_l</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># we only implement 1/m for the L (final layer)</span>
        
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">:</span>
        <span class="n">A_l</span> <span class="o">=</span> <span class="n">sigmoid_forward</span><span class="p">(</span><span class="n">Z_l</span><span class="p">)</span>
        <span class="n">dZ_l</span> <span class="o">=</span> <span class="n">dA_l</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A_l</span><span class="p">)</span> <span class="o">*</span> <span class="n">A_l</span>
        <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ_l</span><span class="p">,</span> <span class="n">W_l</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dZ_l</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have completed a single 'abstract' backward step, we need to put them all into an $L$.</p>
<p>From the get go we are given $A_L$, since that's the output of our forward prop. Next we need to calculate $dA_L$ and $dZ_L$ to pass it to our for loop:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">L_layer_backward</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;categorical&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a full L-layer backward propagation</span>
<span class="sd">    inputs:</span>
<span class="sd">        - A_L - a final output layer - aka predictions</span>
<span class="sd">        - Y - true values the model was predicting</span>
<span class="sd">        - caches - contains all caches from forward propagation steps</span>
<span class="sd">        - loss - is an indicator what type of loss function we should be using to obtain cost, and first derivative dA_L</span>
<span class="sd">    output:</span>
<span class="sd">        - grads - at each layer for W and b</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">A_L</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># after this line, Y is the same shape as A_L</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;categorical&#39;</span><span class="p">:</span>
        <span class="n">dA_L</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">A_L</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">A_L</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;continuous&#39;</span><span class="p">:</span>
        <span class="n">dA_L</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>

    <span class="n">cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># because caches is a list from 0 to L-1</span>
    <span class="n">A_prev</span><span class="p">,</span> <span class="n">W_l</span><span class="p">,</span> <span class="n">b_l</span><span class="p">,</span> <span class="n">Z_l</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dZ_l</span> <span class="o">=</span> <span class="n">single_backward</span><span class="p">(</span><span class="n">dA_L</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>

    <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A_prev</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ_l</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ_l</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># same as dZ_1 dot 1</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span> <span class="c1"># we have L layers, above we did L, so there are L-1 left to go (0 to L-2)</span>
        <span class="n">A_prev</span><span class="p">,</span> <span class="n">W_l</span><span class="p">,</span> <span class="n">b_l</span><span class="p">,</span> <span class="n">Z_l</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
        <span class="n">dA_l</span> <span class="o">=</span> <span class="n">dA_prev</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dZ_l</span> <span class="o">=</span> <span class="n">single_backward</span><span class="p">(</span><span class="n">dA_l</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A_prev</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ_l</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ_l</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># same as dZ_l dot 1</span>
        
    
    <span class="k">return</span> <span class="n">grads</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Testing:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">AL</span><span class="p">,</span> <span class="n">Y_assess</span><span class="p">,</span> <span class="n">caches</span> <span class="o">=</span> <span class="n">L_model_backward_test_case</span><span class="p">()</span>
<span class="n">A_L</span> <span class="o">=</span> <span class="n">AL</span><span class="o">.</span><span class="n">T</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y_assess</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># AN&#39;s cache is cache = [A_prev, W, b]</span>
<span class="c1"># Our cache is cache = [A_prev, W_l, b_l, Z_l, activation]</span>
<span class="n">new_caches</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">new_caches</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> 
             <span class="o">+</span> <span class="n">caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">])</span>
<span class="n">new_caches</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> 
             <span class="o">+</span> <span class="n">caches</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">])</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">L_layer_backward</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">new_caches</span><span class="p">)</span>
<span class="n">grads</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;dW_2&#39;: array([[-0.21308814],
        [-0.12855076],
        [-0.13960917]]),
 &#39;db_2&#39;: array([0.15887873]),
 &#39;dW_1&#39;: array([[ 0.3127498 , -0.3433267 ,  0.        ],
        [ 0.0580888 , -0.06376802,  0.        ],
        [ 0.10466449, -0.11489732,  0.        ],
        [ 0.07975539, -0.08755291,  0.        ]]),
 &#39;db_1&#39;: array([-0.16267208,  0.1785762 ,  0.        ])}</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong></p>
<table style="width:60%">

  <tr>
    <td> dW1 </td> 
           <td> [[ 0.3127498 , -0.3433267 ,  0.        ]
 [ 0.0580888 , -0.06376802,  0.        ]
 [ 0.10466449, -0.11489732,  0.        ]
 [ 0.07975539, -0.08755291,  0.        ]] </td> 
  </tr> 

   <tr>
    <td> db1 </td> 
           <td> [[-0.16267208,  0.1785762 ,  0.        ]] </td> 
  </tr> 

</table>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.3-Updating-Parameters">2.3 Updating Parameters<a class="anchor-link" href="#2.3-Updating-Parameters"> </a></h3><p>Last step in our NN from scratch is to update our parameters: $W$ and $b$! Time to implement the Gradient Descent. Remember that what we did in backward propagation is identify the <strong>direction</strong> in which we should be changing our weights to minimize loss. We still need to know how large of a step we should make in that direction. Here we introduce <strong>learning rate</strong> hyperparameter - controling the magnitude of change.</p>
<p>On the one hand, you want to minimize the loss as fast as possible, and it may be tempting to choose a high learning rate. THe issue is that if you choose it too high - then you may find yourself in a bowl situation. Meaning that you would be swining from one end of the bowl to the next, passing the minimum each time because your step is too big. In some cases, if your step it too high, then your can actually diverge away from the minimum. This hyperparameter often times needs a lot of tuning.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates parameters of our Neural Network utilizing gradients found from the backpropagation step as direction of change</span>
<span class="sd">    and learning_rate as a hyperparameter as the magnitude of change. Outputs an updates set of weights - ready to be used</span>
<span class="sd">    in the next iteration of learning in our Neural Network.</span>
<span class="sd">    </span>
<span class="sd">    input</span>
<span class="sd">        - parameters - weights, W&#39;s, and bias, b&#39;s, for each layer l</span>
<span class="sd">        - grads - gradients found in backpropagation step indicating direction of change</span>
<span class="sd">        - learning_rate - hyperparameter indicating the magnitude of change</span>
<span class="sd">    output</span>
<span class="sd">        - new_parameters - updated weights and bias terms for each layer l</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">new_parameters</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">new_parameters</span><span class="p">[</span><span class="s1">&#39;W_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">new_parameters</span><span class="p">[</span><span class="s1">&#39;b_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    
    <span class="k">return</span> <span class="n">new_parameters</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">update_parameters_test_case</span><span class="p">()</span>
<span class="n">params_</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">params_</span><span class="p">[</span><span class="n">parameter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">parameter</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>
<span class="n">grads_</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">:</span>
    <span class="n">grads_</span><span class="p">[</span><span class="n">grad</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="n">grad</span><span class="p">]</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">params_</span><span class="p">,</span> <span class="n">grads_</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;W_1 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W_1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;b_1 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b_1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;W_2 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W_2&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;b_2 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b_2&quot;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>W_1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b_1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W_2 = [[-0.55569196  0.0354055   1.32964895]]
b_2 = [[-0.84610769]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Expected Output</strong>:</p>
<table style="width:100%"> 
    <tr>
    <td> W1 </td> 
           <td> [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> 
  </tr> 

   <tr>
    <td> b1 </td> 
           <td> [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]] </td> 
  </tr> 
  <tr>
    <td> W2 </td> 
           <td> [[-0.55569196  0.0354055   1.32964895]]</td> 
  </tr> 

    <tr>
    <td> b2 </td> 
           <td> [[-0.84610769]] </td> 
  </tr> 
</table>
</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/blog/2021/01/31/BYO-Artificial-Neural-Network.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This blog post is a way to record my ML and AI journey. It includes longer technical writeups of the projects, as well as shorter but more theoretical posts outlining my understnading or intuition of some concepts. I hope it can be helpful to people who are also on a similar journey!</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nikita-yatchenko" title="nikita-yatchenko"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
