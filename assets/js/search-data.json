{
  
    
        "post0": {
            "title": "Artificial Neural Network (ANN) From Scratch",
            "content": "1. Defining a Structure of our Neural Network . 1.1 Simplified Intuition . What is a neural network? A neural network is a collection of connected nodes, which loosely remind a structure of a human brain. When receiving an input (a signal) the network passes it to a node, which processes it and passes processed signal to a next neuron. Each time we pass processed input to a new neuron, we learn something new about about our input. This information can then be used to better understand a relationship between an input and an output. Let us assume that we have some observed input and for each input we have some output. Neural Network, can desribe the nature of the relationship between the input and an ouput, learning both linear and non-linear components of that relationship. Neural Networks help us find linear and non-linear relationship between an input, $X$, and an output, $Y$. $$Y = f(X)$$ . To make things very simple, imagine your input as a single number $x$. When we use this input in our network, we pass $x$ to a neuron, call it $a$. It will $process$ the input, $x$, to, hopefully, produce an insight from that input, which will allow us to learn something of value about our output $y$. Let us call a processed input from a neuron (aka the insight) $a_l$, where $l$ is a layer of our NN, and define it as follows: $a_l = f_l(w_{l} * x + b_{l})$. Let us call the linear transformation step $z_l = w_{l} * x + b_{l}$ - simply doing that will be equivalent to performing a very large OLS (if there are multiple neurons). The advantage of using a neural network is that it allows us to capture not only a linear relatinoship (OLS) between an input and an output, but also a non-linear one. That is why we have $f_l$ function applied to calculating $a_l$. This allows us to approximate a behavior of infinitely complex functions, given, enough of neurons and layers of neurons are used. . 1.2 General Case . To translate an above simplified explanation to a matrix format, let us first define what an input is, $X$. This matrix is comprised of rows and columns. Where each row is an observation, and each column is a particular feature. Columns can be words and rows can be sentences, columns can be house features (like, distance from center, number of bedrooms, etc.) and rows can be individual houses. . **Figure 1** What a Neural Netork does is it takes features ($x_1$, $x_2$,..$x_n$ - columns) and those vectors become the input. At each layer of the network, the network &#39;learns&#39; new patterns from the input, by means of combining various features together - $z_l = w_{l} * x_i + b_{l}$ and applying a non-linear function to detect non-linear patterns: $a_l = f_l(x_i * w_{l} + b_{l})$, where an example of $f_l()$ could be $ReLU$ function. For the general case there can be multiple layers, $l$. Each layer consists of multiple neurons $a_l$&#39;s that are stacked in that layer. To get from layer $l$ to layer $l+1$ in the network, we treat $a_l$&#39;s as inputs and $a_{l+1}$ as our outputs - and repeat the process. THe intuition behind doing this is as we move up the layers of our Network we learn more complex patterns - details of that relationship. . **Figure 2** Note that the dimensions of each layer are not necessarily the same: $n ?= h ?= k ?= l$. That&#39;s because it is up to us, the architects of the Neural Network to determine the best structure, depending on the problem. . Now, we have input observations, $X$, and output observations, $Y$. The training of the Neural Network is figuring out the most optimal weights, $w_a$&#39;s above. Those weights are the weights for the various features, that will become new features in the next layer. . How do we figure out weights? Well, just like a child learning how to walk, our Neural Network will attempt to predict out output with random guess, and will most likely fail. BUT we will gather important information - a measure of difference between our output $Y$, and what our network predicted $ hat{Y}$ - called the loss. We can then figure out how to vary the weights, $W_l$&#39;s, such that we minimize the loss. In other words, at each iteration of training we find the derivative of the loss function with respect to each $W_l$ and $b_l$, those derivatives show the direction of greatest increase of the loss function. Therefore, for the next iteration, we are going to take negative of that derivative - to make it a direction of steepest descent - and make some step (learning rate) in that direction. . We figure out those weights by employing Stochastic Gradient Descent (SGD) algorithm. SGD is an involved topic, and we can spend a whole new notebook on it. THe main idea of the SGD is that it. . This was a very simple and intuitive overview of Neural Networks. The above is a very crude way to sum up WHAT Neural Networks do and HOW they accomplish it. For each sentence there are blogs, books and tutorials. . 1.3 Outline of This Project. Implement L-layer NN . Again the goal is to implement our own Artificial Neural Network in Python. Here is the basic outline: . Initialize the parameters | Propagation Compute Forward Propagation | Copmute the Loss Function | Compute Backward Propagation | | Update parameters | 2. Coding . 2.1 Initialize the parameters . To begin with, we are going to initialize our parameters. There is a lot of theory behind how to properly initialize parameters, but the main conclusion is that $b$ can be initialized to a vector of all 0&#39;s, but $W$ cannot, otherwise, the NN will not be able to break symmetry and no learning will be done. . Let us next deal with the dimensions of our $W$ and $b$ parameters. For some layer $l$, we have $W_l$, which takes observations in the space of $R_{l-1}$ and transforms them to $R_{l}$. Consider the first step of neural network, input to the first layer: $$ A1_{m, h} = X_{m, n} times W1_{n, h} + b_{1, h} $$, where $m$ is the number of observations, $n$ is the number of features in the input layer, h is the number of features in the first layer, $W1$ has dimension $(n, h)$, aka old dimensions to new dimensions . import numpy as np import math from testCases_v4a import * # testing taken from Andrew Ng&#39;s course . As input we are given a list , containins the depth of each layer. Thus the length of the list is the total number of layers, and the value of each element is the depth (number of new features, aka h). . def initialize_parameters(layers_list): &quot;&quot;&quot; input: - layer_list - list containing the depth of each layer notes: - W_l - has dimensions (layers_list[l-1], layers_list[l]) - b_l - has dimensions (1, layers_list[l]) &quot;&quot;&quot; np.random.seed(3) parameters = {} L = len(layers_list) for l in range(1, L): parameters[&#39;W_&#39; + str(l)] = np.random.randn(layers_list[l-1], layers_list[l]) * 0.01 # need to multiply by a small number to avoid exploding gradients parameters[&#39;b_&#39; + str(l)] = np.zeros([1, layers_list[l]]) return parameters . initialize_parameters([5,4,3]) . {&#39;W_1&#39;: array([[ 0.01788628, 0.0043651 , 0.00096497, -0.01863493], [-0.00277388, -0.00354759, -0.00082741, -0.00627001], [-0.00043818, -0.00477218, -0.01313865, 0.00884622], [ 0.00881318, 0.01709573, 0.00050034, -0.00404677], [-0.0054536 , -0.01546477, 0.00982367, -0.01101068]]), &#39;b_1&#39;: array([[0., 0., 0., 0.]]), &#39;W_2&#39;: array([[-0.01185047, -0.0020565 , 0.01486148], [ 0.00236716, -0.01023785, -0.00712993], [ 0.00625245, -0.00160513, -0.00768836], [-0.00230031, 0.00745056, 0.01976111]]), &#39;b_2&#39;: array([[0., 0., 0.]])} . 2.2 Propagation . Now, after we have initialized our parameters we need to propagate forward to calculate our error (loss) and then we need to propagate backward to calculate gradients at each step. We will use those gradients to modify our parameters. . . **Figure 3** . 2.2.A Forward Propagation . First, we implement forward propagation . Forward Propagation: . $Z_l = A_{l-1} times W_l + b_l$ - this is the linear part of the neuron calculation | $A_l = f_l(Z_l)$ - this is the non-linear part of the neuron calculation. $f_l$ most often are ReLU or sigmoid | To correctly implement backward propagation later on, we need to cache some intemediate values for later use. Namely, we need to cache: $Z_l$, $A_{l-1}$, $W_l$, and $b_l$. . Before we jump into forward propagation, let&#39;s quickly define ReLU (Rectified Linear Unit) and sigmoid functions. ReLU takes only the positive part of the array, aka $max(0,Z)$. While sigmoid is this: $ frac{1}{1+e^{-Z}}$ . def relu_forward(Z_l): &quot;&quot;&quot; Implements a forward activate function ReLU input: - Z_l - a linear part of the neuron calculation output: - A_l - a nonlinear part of the neuron calculation, after applying relu activation function to Z_curr - cache - cached value of Z_curr to be used later in the backward propagation step &quot;&quot;&quot; A_l = np.maximum(0, Z_l) return A_l # print(relu_forward(np.array([ -1, 4, 5, -40]))) def sigmoid_forward(Z_l): &quot;&quot;&quot; Implements a forward activate function sigmoid input: - Z_l - a linear part of the neuron calculation output: - A_l - a nonlinear part of the neuron calculation, after applying sigmoid activation function to Z_l - cache - cached value of Z_l to be used later in the backward propagation step &quot;&quot;&quot; A_l = 1 / (1+ np.exp(-Z_l)) return A_l # print(sigmoid_forward(np.array([ -1, 4, 5, -40]))) . As was outline in Forward Propagation section above, we just need to implement that in code and cache important intermidiate values: . def forward_propagation(A_prev, W_l, b_l, activation = &#39;relu&#39;): &quot;&quot;&quot; Applies a single step of forward propagation, taking A_{l-1} and producing A_l input: - A_prev - previous layer - W_l, b_l - weights for the current to next layer - activation - activation function for the layer output: - A_curr - current layer - cache - storing A_prev, W_l, b_l, Z_l for later use in back prop &quot;&quot;&quot; Z_l = np.dot(A_prev, W_l) + b_l if activation == &#39;relu&#39;: A_curr = relu_forward(Z_l) if activation == &#39;sigmoid&#39;: A_curr = sigmoid_forward(Z_l) cache = [A_prev, W_l, b_l, Z_l, activation] return A_curr, cache . A_prev = np.array([[-0.41675785, -2.1361961 , -1.79343559], [-0.05626683, 1.64027081, -0.84174737]]) W = np.array([[ 0.50288142], [-1.24528809], [-1.05795222]]) b = np.array([[-0.90900761]]) forward_propagation(A_prev, W, b, activation = &#39;sigmoid&#39;) . (array([[0.96890023], [0.11013289]]), [array([[-0.41675785, -2.1361961 , -1.79343559], [-0.05626683, 1.64027081, -0.84174737]]), array([[ 0.50288142], [-1.24528809], [-1.05795222]]), array([[-0.90900761]]), array([[ 3.43896134], [-2.08938436]]), &#39;sigmoid&#39;]) . Note that this is a single forward propagation step, for the full pass through the model we need to loop through the entire structure. This is exactly what we will implement now: . def L_layer_forward(X, parameters): &quot;&quot;&quot; Applies forward propagation for the entire network input: - X - training feature data - parameters - all initialized parameters output: - A_last - last layer activated neurons - caches - a list of caches for each layer of the network &quot;&quot;&quot; caches = [] L = len(parameters) // 2 A = X for l in range(1, L): W = parameters[&#39;W_&#39;+str(l)] b = parameters[&#39;b_&#39;+str(l)] A_prev, cache = forward_propagation(A, W, b, activation = &#39;relu&#39;) caches.append(cache) A = A_prev W = parameters[&#39;W_&#39;+str(L)] b = parameters[&#39;b_&#39;+str(L)] A_L, cache = forward_propagation(A, W, b, activation = &#39;sigmoid&#39;) caches.append(cache) assert(A_L.shape == (X.shape[0], 1)) return A_L, caches . parameters = initialize_parameters([4, 4, 3, 1]) . X = np.array([[-0.31178367, 0.72900392, 0.21782079, -0.8990918 ], [-2.48678065, 0.91325152, 1.12706373, -1.51409323], [ 1.63929108, -0.4298936 , 2.63128056, 0.60182225], [-0.33588161, 1.23773784, 0.11112817, 0.12915125], [ 0.07612761, -0.15512816, 0.63422534, 0.810655 ]]) . L_layer_forward(X, parameters)[0] . array([[0.49999998], [0.4999998 ], [0.50000061], [0.5 ], [0.50000011]]) . X, parameters = L_model_forward_test_case_2hidden() X = X.T new_parameters = {} for i in parameters: name = list(i) new_parameters[name[0] + &#39;_&#39; + name[1]] = np.transpose(parameters[i]) AL, caches = L_layer_forward(X, new_parameters) print(&quot;AL = &quot; + str(AL)) print(&quot;Length of caches list = &quot; + str(len(caches))) . AL = [[0.03921668] [0.70498921] [0.19734387] [0.04728177]] Length of caches list = 3 . **AL** | [[ 0.03921668 0.70498921 0.19734387 0.04728177]] | . **Length of caches list ** | 3 | . 2.2.B Cost Function . As we mentioned earlier, the goal is to update parameters, based on how the cost function is reacting to changes in parameters. Therefore, defining a cost function is one of thekeys to having a successful Neural Network. There are many choices of such functions, depending on the problem&#39;s specifications: . If we have a categorization problem, then a good choice can be cross entropy loss, defined as $J = -1/m sum_{i = 0}^{m}( sum_{k = 1}^{K}t_i[k]*log(f(s)_i[k]))$, where $t$ is truth and $s$ is the output of the NN, and m is the total number of observations. | If we have a regression problem ($Y_{true}$ is continuous), then a good choice can be MSE loss, defined as $J = 1/m* sum_{i = 1}^{m}(y_i - hat{y}_i)^2$, where m is the number of observations. | backward propagation&#39;s purpose is to obtain parameters&#39; derivatives. . def compute_cost(A_L, Y, loss = &#39;categorical&#39;): &quot;&quot;&quot; Computes average cost for all training examples using either cross entropy or MSE loss, depending on the specialization input: - A_L - final layer of the network - Y - true y values - loss - categorical or continuous &quot;&quot;&quot; m = len(Y) if loss == &#39;categorical&#39;: cost = -1/m * (np.dot(Y.T, np.log(A_L)) + np.dot(1 - Y.T, np.log(1 - A_L))) if loss == &#39;continuous&#39;: diff = Y - A_L cost = 1/m * np.dot(diff.T, diff)^2 cost = np.squeeze(cost) # To make sure your cost&#39;s shape is what we expect (e.g. this turns [[17]] into 17) return cost . Y, AL = compute_cost_test_case() print(&quot;cost = &quot; + str(compute_cost(AL.T, Y.T))) . cost = 0.2797765635793422 . 2.2.C Backward Propagation . Now for the last part of this exercise - to build a backward propagation to calculate our derivatives. Remember our visual representation of propagation: . **Figure 3** . Let&#39;s approach this first theoretically. Remember that we have the following from our forward propagation step: . $Z_l = A_{l-1} times W_l + b_l$ (Equation 1) | $A_l = f_l(Z_l)$, where $f_l$ is some activation function (in our case ReLU or sigmoid) | After going through the entire network we also have $J$, a cost function, which is a measure of difference between $A_L$ and $Y$, thus $J approx A_L$. | We want to find the following derivatives by applying the chain rule: . $ frac{ delta J}{ delta W_l} = frac{ delta J}{ delta A_L} times frac{ delta A_L}{ delta A_{L-1}} times frac{ delta A_{L-1}}{ delta A_{L-2}} ... times frac{ delta A_{l+1}}{ delta A_{l}} times frac{ delta A_{l}}{ delta Z_{l}} times frac{ delta Z_{l}}{ delta W_{l}} = frac{ delta J}{ delta Z_l} times frac{ delta Z_l}{ delta W_l} = dZ_l times frac{ delta Z_l}{ delta W_l} = dZ_l times A_{l-1}$ | What is $ frac{ delta Z_l}{ delta W_l}$ equal to? From Equation 1 we can find the derivative of $Z_l$ with respect to $W_l$: $ frac{ delta Z_l}{ delta W_l} = frac{ delta (A_{l-1} times W_l + b_l)}{ delta W_l} = A_{l-1} $ . So we need to understand what $dZ_l$ equal to . Now, let&#39;s derive $dZ_l = frac{ delta J}{ delta Z_l} = frac{ delta J}{ delta A_L} times frac{ delta A_L}{ delta A_{L-1}} times frac{ delta A_{L-1}}{ delta A_{L-2}} ... times frac{ delta A_{l+1}}{ delta A_{l}} times frac{ delta A_{l}}{ delta Z_{l}} = frac{ delta J}{ delta A_l} times frac{ delta A_{l}}{ delta Z_{l}} = dA_l * f&#39;_l(Z_l)$ (Equation 2) | Note that $dA_l = frac{ delta J}{ delta A_l}$, $f&#39;_l(Z_l)$ is a derivative of the activation function in layer $l$ evaluated for $Z_l$, and $*$ is a term-by-term multiplication of matricies. . The complicated part from above is $dA_l$. Let&#39;s derive it using chain rule: | $dA_l = frac{ delta J}{ delta A_l} = frac{ delta J}{ delta A_L} times frac{ delta A_L}{ delta A_{L-1}} times frac{ delta A_{L-1}}{ delta A_{L-2}} ... times frac{ delta A_{l+1}}{ delta Z_{l+1}} times frac{ delta Z_{l+1}}{ delta A_{l}} = dZ_{l+1} times frac{ delta Z_{l+1}}{ delta A_{l}} = dZ_{l+1} times W_{l+1}$ . Note that $ frac{ delta Z_{l}}{ delta A_{l-1}} = W_l$, since $Z_l = A_{l-1} times W_l + b_l$ . Using our newly derived $dA_l$ we substitute it back to Equation 2 to obtain: $dZ_l = dZ_{l+1} times W_{l+1} * f&#39;_l(Z_l)$ . $ frac{ delta J}{ delta b_l} = frac{ delta J}{ delta A_L} times frac{ delta A_L}{ delta A_{L-1}} times frac{ delta A_{L-1}}{ delta A_{L-2}} ... times frac{ delta A_{l+1}}{ delta A_{l}} times frac{ delta A_{l}}{ delta Z_{l}} times frac{ delta Z_{l}}{ delta b_{l}} = frac{ delta J}{ delta Z_l} times frac{ delta Z_l}{ delta b_l} = dZ_l times frac{ delta Z_l}{ delta b_l} = dZ_l times 1$, | where $1$ is the identity matrix since $Z_l = A_{l-1} times W_l + b_l$ . 2.2.5.1 Calculation Order . Let&#39;s break this up into several steps. We are half way through backward propagation - at layer $l+1$. We have calculated $dA_{l+1}, dW_{l+1}, db_{l+1}, dZ_{l+1}$. We want to get $dA_{l}, dW_{l}, db_{l}, dZ_{l}$: . Function singlebackward - propagates through one layer in the network using $dZ{l+1}$, $W_{l+1}$ to get $dZ_l$ and $dA_l$, utilizing $f&#39;_l(Z_l)$ . When we have classification with 2 classes, our final activation function is &#39;sigmoid&#39; function, which squeeshes activated neurons to be between values of 0 and 1. But for the rest of the layer we can choose either ReLU or &#39;sigmoid&#39;. . Below is the implementation of the function single_backward. The calculation logic is the same, however, the calculations depend on what activation function was used for that layer. Let&#39;s consider two activation functions: . ReLU - it is an identity function, when the value is greater or equal to $0$, but when the input is less than $0$, the output is $0$. Thus the derivative is $1$ for values greater than $0$, and $0$ for values that are less than $0$. | Sigmoid is usually used as the last activation function for the last layer. If our cost function is cross-entropy and we have a binary classification ($K = 2$), then $J = -1/m sum_{i = 0}^{m}(y_i*log(AL_i) + (1- y_i)*log(1-AL_i)$. Let us first compute derivative calculation on a single observation and then generalize for all by summing over $m$ and taking the average $ frac{1}{m}$. So consider one example: $J_i = -((y_i*log(AL_i) + (1 - y_i)*log(1-AL_i))$, then $ frac{ delta J_i}{ delta AL_i} = frac{AL_i - y_i}{AL_i(1-AL_i)}$ | So on the case of the binary classification $f_L(Z_L) = A_L = frac{1}{1+e^{-Z_L}}$, then $f&#39;_L(Z_L) = ( frac{1}{1+e^{-Z_L}})&#39; = ((1+e^{-Z_L})^{-1})&#39; = -1 times-1 times e^{-Z_L} times frac{1}{(1+e^{-Z_L})^{-2}} = frac{e^{-Z_L}}{1+e^{-Z_L}} times frac{1}{1+e^{-Z_L}} = (1- frac{1}{1+e^{-Z_L}}) times frac{1}{1+e^{-Z_L}} = (1-f_L(Z_L)) times f_L(Z_L) = (1-A_L) times A_L$ . Thus $ frac{ delta J_i}{ delta ZL_i} = frac{ delta J_i}{ delta AL_i} times frac{ delta AL_i}{ delta ZL_i} = frac{AL_i - y_i}{AL_i(1-AL_i)} * f&#39;_L(Z_L) = frac{AL_i - y_i}{AL_i(1-AL_i)} times (1-AL_i) times AL_i = AL_i - y_i$. . Conclusion is that $dZ_L = frac{1}{m} times (A_L - Y)$ for the last layer. . If it is not the last layer, then $dZ_l = dA_l times (1-A_l) times A_l$ . def single_backward(dA_l, cache): &quot;&quot;&quot; Given dA_l, cache - finds the value of dA_prev based on what kind of activation was used. input: - dA_l is the derivative of J with respect to current hidden layer A_curr - cache is a list of A_prev, W_l, b_l, Z_l, activation output: - dA_prev &quot;&quot;&quot; A_prev, W_l, b_l, Z_l, activation = cache if activation == &#39;relu&#39;: relu_deriv = np.full(Z_l.shape, fill_value = 1) relu_deriv[Z_l &lt;= 0] = 0 dZ_l = dA_l * relu_deriv dA_prev = np.dot(dZ_l, W_l.T) # we only implement 1/m for the L (final layer) if activation == &#39;sigmoid&#39;: A_l = sigmoid_forward(Z_l) dZ_l = dA_l * (1-A_l) * A_l dA_prev = np.dot(dZ_l, W_l.T) return dA_prev, dZ_l . Now that we have completed a single &#39;abstract&#39; backward step, we need to put them all into an $L$. . From the get go we are given $A_L$, since that&#39;s the output of our forward prop. Next we need to calculate $dA_L$ and $dZ_L$ to pass it to our for loop: . def L_layer_backward(A_L, Y, caches, loss = &#39;categorical&#39;): &quot;&quot;&quot; Performs a full L-layer backward propagation inputs: - A_L - a final output layer - aka predictions - Y - true values the model was predicting - caches - contains all caches from forward propagation steps - loss - is an indicator what type of loss function we should be using to obtain cost, and first derivative dA_L output: - grads - at each layer for W and b &quot;&quot;&quot; L = len(caches) m = len(Y) Y = Y.reshape(A_L.shape) # after this line, Y is the same shape as A_L assert(Y.shape == (m, 1)) grads = {} if loss == &#39;categorical&#39;: dA_L = 1/m * (-np.divide(Y, A_L) + np.divide(1-Y, 1-A_L)) if loss == &#39;continuous&#39;: dA_L = 1/m * (np.dot(A_L, Y.T)) cache = caches[L-1] # because caches is a list from 0 to L-1 A_prev, W_l, b_l, Z_l, activation = cache dA_prev, dZ_l = single_backward(dA_L, cache) grads[&#39;dW_&#39; + str(L)] = np.dot(A_prev.T, dZ_l) grads[&#39;db_&#39; + str(L)] = np.sum(dZ_l, axis = 0) # same as dZ_1 dot 1 for l in reversed(range(L-1)): # we have L layers, above we did L, so there are L-1 left to go (0 to L-2) A_prev, W_l, b_l, Z_l, activation = caches[l] dA_l = dA_prev dA_prev, dZ_l = single_backward(dA_l, caches[l]) grads[&#39;dW_&#39; + str(l+1)] = np.dot(A_prev.T, dZ_l) grads[&#39;db_&#39; + str(l+1)] = np.sum(dZ_l, axis = 0) # same as dZ_l dot 1 return grads . Testing: . AL, Y_assess, caches = L_model_backward_test_case() A_L = AL.T Y = Y_assess.T # AN&#39;s cache is cache = [A_prev, W, b] # Our cache is cache = [A_prev, W_l, b_l, Z_l, activation] new_caches = [] new_caches.append([caches[0][0][0].T, caches[0][0][1].T, caches[0][0][2].T, np.dot(caches[0][0][0].T, caches[0][0][1].T) + caches[0][0][2].T, &#39;relu&#39;]) new_caches.append([caches[1][0][0].T, caches[1][0][1].T, caches[1][0][2].T, np.dot(caches[1][0][0].T, caches[1][0][1].T) + caches[1][0][2].T, &#39;sigmoid&#39;]) grads = L_layer_backward(A_L, Y, new_caches) grads . {&#39;dW_2&#39;: array([[-0.21308814], [-0.12855076], [-0.13960917]]), &#39;db_2&#39;: array([0.15887873]), &#39;dW_1&#39;: array([[ 0.3127498 , -0.3433267 , 0. ], [ 0.0580888 , -0.06376802, 0. ], [ 0.10466449, -0.11489732, 0. ], [ 0.07975539, -0.08755291, 0. ]]), &#39;db_1&#39;: array([-0.16267208, 0.1785762 , 0. ])} . Expected Output . dW1 | [[ 0.3127498 , -0.3433267 , 0. ] [ 0.0580888 , -0.06376802, 0. ] [ 0.10466449, -0.11489732, 0. ] [ 0.07975539, -0.08755291, 0. ]] | . db1 | [[-0.16267208, 0.1785762 , 0. ]] | . 2.3 Updating Parameters . Last step in our NN from scratch is to update our parameters: $W$ and $b$! Time to implement the Gradient Descent. Remember that what we did in backward propagation is identify the direction in which we should be changing our weights to minimize loss. We still need to know how large of a step we should make in that direction. Here we introduce learning rate hyperparameter - controling the magnitude of change. . On the one hand, you want to minimize the loss as fast as possible, and it may be tempting to choose a high learning rate. THe issue is that if you choose it too high - then you may find yourself in a bowl situation. Meaning that you would be swining from one end of the bowl to the next, passing the minimum each time because your step is too big. In some cases, if your step it too high, then your can actually diverge away from the minimum. This hyperparameter often times needs a lot of tuning. . def update_parameters(parameters, grads, learning_rate): &quot;&quot;&quot; Updates parameters of our Neural Network utilizing gradients found from the backpropagation step as direction of change and learning_rate as a hyperparameter as the magnitude of change. Outputs an updates set of weights - ready to be used in the next iteration of learning in our Neural Network. input - parameters - weights, W&#39;s, and bias, b&#39;s, for each layer l - grads - gradients found in backpropagation step indicating direction of change - learning_rate - hyperparameter indicating the magnitude of change output - new_parameters - updated weights and bias terms for each layer l &quot;&quot;&quot; L = len(parameters) // 2 new_parameters = {} for l in range(L): new_parameters[&#39;W_&#39; + str(l+1)] = parameters[&#39;W_&#39; + str(l+1)] - learning_rate * grads[&#39;dW_&#39; + str(l+1)] new_parameters[&#39;b_&#39; + str(l+1)] = parameters[&#39;b_&#39; + str(l+1)] - learning_rate * grads[&#39;db_&#39; + str(l+1)] return new_parameters . parameters, grads = update_parameters_test_case() params_ = {} for parameter in parameters: params_[parameter[0] + &#39;_&#39; + parameter[1]] = parameters[parameter] grads_ = {} for grad in grads: grads_[grad[:2] + &#39;_&#39; + grad[2]] = grads[grad] parameters = update_parameters(params_, grads_, 0.1) print (&quot;W_1 = &quot;+ str(parameters[&quot;W_1&quot;])) print (&quot;b_1 = &quot;+ str(parameters[&quot;b_1&quot;])) print (&quot;W_2 = &quot;+ str(parameters[&quot;W_2&quot;])) print (&quot;b_2 = &quot;+ str(parameters[&quot;b_2&quot;])) . W_1 = [[-0.59562069 -0.09991781 -2.14584584 1.82662008] [-1.76569676 -0.80627147 0.51115557 -1.18258802] [-1.0535704 -0.86128581 0.68284052 2.20374577]] b_1 = [[-0.04659241] [-1.28888275] [ 0.53405496]] W_2 = [[-0.55569196 0.0354055 1.32964895]] b_2 = [[-0.84610769]] . Expected Output: . W1 | [[-0.59562069 -0.09991781 -2.14584584 1.82662008] [-1.76569676 -0.80627147 0.51115557 -1.18258802] [-1.0535704 -0.86128581 0.68284052 2.20374577]] | . b1 | [[-0.04659241] [-1.28888275] [ 0.53405496]] | . W2 | [[-0.55569196 0.0354055 1.32964895]] | . b2 | [[-0.84610769]] | .",
            "url": "https://nikita-yatchenko.github.io/blog/2021/01/31/BYO-Artificial-Neural-Network.html",
            "relUrl": "/2021/01/31/BYO-Artificial-Neural-Network.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Named Entity Recognition using LSTM in Keras",
            "content": "Introduction . Named Entity Recognition can be used as a standalone tool but also as a preprocessing toolfor later applications in Machine Translation, Customer Feedback Hanlding, and even Text Summarization. . 1. Import Modules . %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np np.random.seed(0) plt.style.use(&quot;ggplot&quot;) import tensorflow as tf print(&#39;Tensorflow version:&#39;, tf.__version__) print(&#39;GPU detected:&#39;, tf.config.list_physical_devices(&#39;GPU&#39;)) . Tensorflow version: 2.3.0 GPU detected: [] . . . 2. Explore NER Dataset . This dataset contains of course sentences in English but they also have corresponding annotations for each word. The sentence in dataset are encoded in Latin 1. . Essential info about entities: . geo = Geographical Entity | org = Organization | per = Person | gpe = Geopolitical Entity | tim = Time indicator | art = Artifact | eve = Event | nat = Natural Phenomenon | . Total Words Count = 1354149 . Target Data Column: “tag” . data = pd.read_csv(&quot;data/ner_dataset.csv&quot;, encoding=&quot;latin1&quot;) data = data.fillna(method=&quot;ffill&quot;) data.head(25) . Sentence # Word POS Tag . 0 Sentence: 1 | Thousands | NNS | O | . 1 Sentence: 1 | of | IN | O | . 2 Sentence: 1 | demonstrators | NNS | O | . 3 Sentence: 1 | have | VBP | O | . 4 Sentence: 1 | marched | VBN | O | . 5 Sentence: 1 | through | IN | O | . 6 Sentence: 1 | London | NNP | B-geo | . 7 Sentence: 1 | to | TO | O | . 8 Sentence: 1 | protest | VB | O | . 9 Sentence: 1 | the | DT | O | . 10 Sentence: 1 | war | NN | O | . 11 Sentence: 1 | in | IN | O | . 12 Sentence: 1 | Iraq | NNP | B-geo | . 13 Sentence: 1 | and | CC | O | . 14 Sentence: 1 | demand | VB | O | . 15 Sentence: 1 | the | DT | O | . 16 Sentence: 1 | withdrawal | NN | O | . 17 Sentence: 1 | of | IN | O | . 18 Sentence: 1 | British | JJ | B-gpe | . 19 Sentence: 1 | troops | NNS | O | . 20 Sentence: 1 | from | IN | O | . 21 Sentence: 1 | that | DT | O | . 22 Sentence: 1 | country | NN | O | . 23 Sentence: 1 | . | . | O | . 24 Sentence: 2 | Families | NNS | O | . The column of interest is the rightmost one - where we see the tag for each word. Let&#39;s check out the number of unique words in the corpus and number of unique tags: . print(&quot;Unique words in corpus:&quot;, data[&#39;Word&#39;].nunique()) print(&quot;Unique tags in corpus:&quot;, data[&#39;Tag&#39;].nunique()) . Unique words in corpus: 35178 Unique tags in corpus: 17 . Create a list of unique words . words = list(set(data.Word.values)) words.append(&quot;ENDPAD&quot;) # end of sentence ? num_words = len(words) . words[0] . &#39;plucked&#39; . Similar process for target variable (tags) . tags = list(set(data.Tag.values)) num_tags = len(tags) . Now, we are gonna modify our dataset so that we can easily split our dataset into our feature matrix and the target vector. So we want to create two pools (containing 3 values) for each sentence so that the 1st value in the two pools is the word and the 2nd value is the POS(Part of Speech) and 3rd is Tag i.e. class name. . 3. Retrieve Sentences and Corresponding Tags . Let&#39;s retreive sentences and their corresponding tags in a nice format in order to have a clear input and a clear output for our Recurrent Neural Network. . First, we group given data by sentences. Then we apply lambda function that extracts Word, POS, and TAG for each grouped sentence to create a list of tuples with a structure: (Word to POS to TAG). . class SentenceGetter(): def __init__(self, data): self.n_sent = 1 self.data = data self.empty = False agg_func = lambda s : [(w, p, t) for w, p, t in zip(s[&#39;Word&#39;].values.tolist(), s[&#39;POS&#39;].values.tolist(), s[&#39;Tag&#39;].values.tolist())] self.grouped = self.data.groupby(&#39;Sentence #&#39;).apply(agg_func) self.sentences = [s for s in self.grouped] def get_next(self): try: s = self.grouped[&quot;Sentence: {}&quot;.format(self.n_sent)] self.n_sent += 1 return s except: return None . getter = SentenceGetter(data) sentences = getter.sentences . Observe that the first sentence countains the exact information we wanted: . sentences[0] . [(&#39;Thousands&#39;, &#39;NNS&#39;, &#39;O&#39;), (&#39;of&#39;, &#39;IN&#39;, &#39;O&#39;), (&#39;demonstrators&#39;, &#39;NNS&#39;, &#39;O&#39;), (&#39;have&#39;, &#39;VBP&#39;, &#39;O&#39;), (&#39;marched&#39;, &#39;VBN&#39;, &#39;O&#39;), (&#39;through&#39;, &#39;IN&#39;, &#39;O&#39;), (&#39;London&#39;, &#39;NNP&#39;, &#39;B-geo&#39;), (&#39;to&#39;, &#39;TO&#39;, &#39;O&#39;), (&#39;protest&#39;, &#39;VB&#39;, &#39;O&#39;), (&#39;the&#39;, &#39;DT&#39;, &#39;O&#39;), (&#39;war&#39;, &#39;NN&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;IN&#39;, &#39;O&#39;), (&#39;Iraq&#39;, &#39;NNP&#39;, &#39;B-geo&#39;), (&#39;and&#39;, &#39;CC&#39;, &#39;O&#39;), (&#39;demand&#39;, &#39;VB&#39;, &#39;O&#39;), (&#39;the&#39;, &#39;DT&#39;, &#39;O&#39;), (&#39;withdrawal&#39;, &#39;NN&#39;, &#39;O&#39;), (&#39;of&#39;, &#39;IN&#39;, &#39;O&#39;), (&#39;British&#39;, &#39;JJ&#39;, &#39;B-gpe&#39;), (&#39;troops&#39;, &#39;NNS&#39;, &#39;O&#39;), (&#39;from&#39;, &#39;IN&#39;, &#39;O&#39;), (&#39;that&#39;, &#39;DT&#39;, &#39;O&#39;), (&#39;country&#39;, &#39;NN&#39;, &#39;O&#39;), (&#39;.&#39;, &#39;.&#39;, &#39;O&#39;)] . 4. Define Mapping Between Words and Numbers; Between Tags and Numbers . To build an RNN we need to represent words and tags as vectors, or at least as numerical representations (values or indecies). . word2idx = {w : i for i, w in enumerate(words)} tag2idx = {t : i for i, t in enumerate(tags)} . import pandas as pd pd.DataFrame.from_dict(word2idx, orient = &#39;index&#39;).tail() . 0 . Socialists 35174 | . bolstering 35175 | . One-third 35176 | . author 35177 | . ENDPAD 35178 | . We can retrieve these words using their indices and looking them up in the dictionary and returing the corresponding keys. . 5. Padding Input Sentences and Creating Train / Test Splits . To build a Recurrent Neural Netowork we need to be able to use equal length sentences (technical necessity of Keras / Tensorflow packages). Thus we are goin to pad our sentences to a prescribed length. So what will this length be? Let us first look at the distribution of sentences&#39; length - look at the histogram. . print(&quot;Total number of sentences = &quot; + str(len(sentences))) _ = plt.hist([len(s) for s in sentences], bins = 50) print(&quot;Mean of the distribution is &quot; + str(sum([len(s) for s in sentences])/ len(sentences))) print(&quot;IQ range (10% to 99%) of the distribution is &quot; + str(np.percentile([len(s) for s in sentences], [10, 99]))) . Total number of sentences = 47959 Mean of the distribution is 21.863987989741236 IQ range (10% to 99%) of the distribution is [12. 43.] . Looking at the plot, considering the mean, the IQ ranges we can observe that maxing out at the length 50 should capture a vast moajority of sentences. . 5.1 Padding . X is going to be a numerical representation of our words for each sentence (we could potentially use a far better word to vector embedding). Iterate over each word to get a corresponding value for that word. . To pad we will use pad_sequence function from the Tensorflow package, where we will specify the maxlen parameter to indicate maximum length each sentences will have and a value parameter - what we are padding with. . from tensorflow.keras.preprocessing.sequence import pad_sequences max_len = 50 X = [[word2idx[w[0]] for w in s] for s in sentences] X = pad_sequences(maxlen = max_len, sequences = X, padding = &#39;post&#39;, value=num_words-1) y = [[tag2idx[w[2]] for w in s] for s in sentences] y = pad_sequences(maxlen = max_len, sequences = y, padding = &#39;post&#39;, value=tag2idx[&#39;O&#39;]) . Let&#39;s split the dataset into training and testing to test our final model&#39;s performance. Let&#39;s set the test size at 20%. . from sklearn.model_selection import train_test_split X_train, X_test, _train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1) . 6. Bidirectional LSTM Model . In this section we are going to build an architecture for the predictive model. Some of the questions pertaining to the model&#39;s architecture: . Why use Bidirectional model? . Bidirectional model allows us to make use not only of the past information but also of the future information. Meaning, if we are at node 5 (word 5), we not only use information about previous 4 words, but also take into account what relationships are present in words 6 - 50. | . Why use LSTM and not just simple RNN? . Regular RNN has a few major drawbacks: vanishing moments, and they are heavily influenced by close dependencies. LSTM addresses both problems by indroducing a &quot;memory cell&quot;, which better stores information from the past and at each next node, LSTM considers whether to use more current information or weigh in more towards the past. | unites are ~ channels (think of various interpretations / features of that particular spot in a sequence) | so LSTM can either return (N observations, time-steps, features / channels) | return_sequences = True (many-to-many) has the effect of each LSTM unit returning a sequence of y_lenth outputs (aka time-steps != 1), one for each time step in the input data, instead of single output value if return_sequences = False (many-to-one). | . helpful links: (https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50235563#50235563, https://datascience.stackexchange.com/questions/10836/the-difference-between-dense-and-timedistributeddense-of-keras, ind helpful: https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/) . Why use Embedding layer? . In the Embedding layer words are represented by dense vectors where a vector represents the projection of the word (input vector space) into a continuous vector space (output vector space). | . Why use TimeDistributed layer? . Applies something (we specify this to be a dense layer) to each time step vector individually. Distribute dense layer calculation across time! If we get 700 (h_1, h_2, ... h_700) LSTM vectors (1 - observation, time-step, 200 - channels) - with the dense layer we collapse it to (1, 1) - for the first observation at time-step 1 | . What is and why use SpatialDropout? . Drops entire 1D feature maps instead of individual elements (used in convolution). So after embedding we have each word represented by a 1x50 vector. SpatialDropout will then take 10% of those channels and zero them out across ALL words. This is done to avoid co-adaptpation - forming a dependency group from one channel to another - a regularization method https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it | . from tensorflow.keras import Model, Input from tensorflow.keras.layers import LSTM, Embedding, Dense from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional . Let us first define an input shape. In keras we use Input to tell our model what object size is going through its pipeline. In this case, we are going to have an object of shape max length of a sentence, currently set to 50. Comma is there to indicate that it is not an integer, but a shape of an array. . Then we go on to the Embedding layer. This layer creates dense layers from sparse word vectors using word2vec or GloVe algorithms / methodologies. We can again think of this as creating new channels to represent word (not necessaryly, but each channel can tell us the word&#39;s plurality or singularity, masculine or feminine, has some connotation estimation as so on). . In order to avoid overfitting by creating dependencies between those channels (let&#39;s say the model has picked up that combining channel 30 with channel 47 produces lower loss), we break those dependencies by introducing a SpatialDropout layer, which nulls out the entire channel. . Next, we go on to creating Bidirection LSTM layer - LSTM&#39;s units - in the output # of channels - I think of it as different insights LSTM got from analyzing data, sort of like new features or hidden units in a regular NN. We must declare return_sequences = True, as we are interested in obtaining output vectors for every time step (many-to-many). . Finally, we need to convert Bidirectional LSTM&#39;s channels into tags, thus we need to apply a simple Dense layer to EACH time step. That is why we use Time-distributed layer. It applies Dense layer (with the same weights) to ALL time steps to produce a singular output - Tag with the highest probability after softmax. . input_word = Input(shape=(max_len,)) # batch of max_len dimensional vectors # model model = Embedding(input_dim = num_words, output_dim = 50, input_length = max_len)(input_word) model = SpatialDropout1D(0.1)(model) # drops entire 1D feature maps instead of individual elements model = Bidirectional(LSTM(units = 100, return_sequences = True, recurrent_dropout = 0.1))(model) # units - dimensions of the output 1 word (after embedding 1 x 250) becomes (1 x 100) after LSTM out = TimeDistributed(Dense(num_tags, activation = &#39;softmax&#39;))(model) # probability of each word being eah tag model = Model(input_word, out) model.summary() . Model: &#34;functional_9&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_6 (InputLayer) [(None, 50)] 0 _________________________________________________________________ embedding_5 (Embedding) (None, 50, 50) 1758950 _________________________________________________________________ spatial_dropout1d_4 (Spatial (None, 50, 50) 0 _________________________________________________________________ bidirectional_5 (Bidirection (None, 50, 200) 120800 _________________________________________________________________ time_distributed_5 (TimeDist (None, 50, 17) 3417 ================================================================= Total params: 1,883,167 Trainable params: 1,883,167 Non-trainable params: 0 _________________________________________________________________ . The model produces a total of 1,883,167 parameters. How exactly are they calculated? Let&#39;s find out: . # gate and cell state calculation we have a set of 2 matricies: matrix that deals with a hidden unit at time step t-1 and matrix that # deals with an input x_t. So we have 4 sets of parameters and each has 2 matricies: lstm_num_params = 4 * (100 * 50 + 100 * 100 + 100) print(&#39;Number of LSTM parameters: &#39;, lstm_num_params) # Now deal with the bidirectional part. We are going to end up having 2 hidden unit from 2 directions: forward and backward. print(&#39;Number of Bidirection LSTM parameters: &#39;, 2 * lstm_num_params) # time distributed / dense layer: output_size * (input_size + 1) - because of the bias + 1 dense_num_params = 2 * 17 * (100) + 17 # maybe same bias unit is applied to both forward and backward print(&#39;Number of Time-Distributed Dense parameters: &#39;, dense_num_params) . Number of LSTM parameters: 60400 Number of Bidirection LSTM parameters: 120800 Number of Time-Distributed Dense parameters: 3417 . Next, let&#39;s compile a model. We are using the most popular optimizer algorithm - Adam, out loss function is sparse categorical cross entropy (classes are mutually exclusive), and out metric is accuracy . model.compile(optimizer = &#39;adam&#39;, loss = &#39;sparse_categorical_crossentropy&#39;, metrics=[&quot;accuracy&quot;]) # what is sparse_categorical_crossentropy . Time to train and test our model. By importing ModelCheckpoint we can save the best model, monitor our calidation loss. Early Stopping allows us to early stop when we do not continue to improve on our model after 2 batches. Finally, PlotLossesCallback is a wayt to visualize our loss. . from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping from livelossplot.inputs.tf_keras import PlotLossesCallback . chkpt = ModelCheckpoint(&quot;model_weights.h5&quot;, monitor=&#39;val_loss&#39;,verbose=1, save_best_only=True, save_weights_only=True, mode=&#39;min&#39;) early_stopping = EarlyStopping(monitor=&#39;val_accuracy&#39;, min_delta=0, patience=2, verbose=0, mode=&#39;max&#39;, baseline=None, restore_best_weights=False) my_callbacks = [PlotLossesCallback(), chkpt, early_stopping] history = model.fit( x=X_train, y=y_train, validation_data=(X_test,y_test), batch_size=32, epochs=3, verbose=1, callbacks = my_callbacks ) . accuracy training (min: 0.958, max: 0.989, cur: 0.989) validation (min: 0.981, max: 0.985, cur: 0.985) Loss training (min: 0.037, max: 0.181, cur: 0.037) validation (min: 0.049, max: 0.068, cur: 0.049) Epoch 00003: val_loss improved from 0.05168 to 0.04879, saving model to model_weights.h5 1199/1199 [==============================] - 133s 111ms/step - loss: 0.0374 - accuracy: 0.9885 - val_loss: 0.0488 - val_accuracy: 0.9853 . model.evaluate(X_test, y_test) . 300/300 [==============================] - 4s 14ms/step - loss: 0.0488 - accuracy: 0.9853 . [0.04879415035247803, 0.9853190183639526] . i = np.random.randint(0, X_test.shape[0]) # 659 p = model.predict(np.array([X_test[i]])) p = np.argmax(p, axis=-1) y_true = y_test[i] print(&quot;{:15}{:5} t {} n&quot;.format(&quot;Word&quot;, &quot;True&quot;, &quot;Pred&quot;)) print(&quot;-&quot; *30) for w, true, pred in zip(X_test[i], y_true, p[0]): print(&quot;{:15}{} t{}&quot;.format(words[w], tags[true], tags[pred])) . Word True Pred She O O remains O O hospitalized O O . O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O ENDPAD O O .",
            "url": "https://nikita-yatchenko.github.io/blog/2021/01/21/Named-Entity-Recognition-using-LSTM-in-Keras.html",
            "relUrl": "/2021/01/21/Named-Entity-Recognition-using-LSTM-in-Keras.html",
            "date": " • Jan 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Nikita Yatchenko and I am passionate about mathematics and machine learning. My interest in these disciplines can be summarized as applying abstract mathematical thinking to dissect complex problems into meaningful insights. I am curious about novel technology that helps me uncover my potential in this interest. . With this blog it is my goal to record my journey as I grow as data scientist. This will primarily be a record of the projects that I complete and insights that I have gathered from them. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nikita-yatchenko.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nikita-yatchenko.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}